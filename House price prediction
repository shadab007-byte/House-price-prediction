import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# Load the dataset
data = pd.read_csv('bengaluru_house_data.csv')

# Data Preprocessing
data.fillna(data.mean(), inplace=True)

label_encoders = {}
categorical_columns = data.select_dtypes(include=['object']).columns
for col in categorical_columns:
    le = LabelEncoder()
    data[col] = le.fit_transform(data[col])
    label_encoders[col] = le

# Features and target
features = data.drop('Price', axis=1)
target = data['Price']

# Normalization
scaler = StandardScaler()
features_scaled = scaler.fit_transform(features)

# Splitting the dataset
X_train, X_test, y_train, y_test = train_test_split(features_scaled, target, test_size=0.2, random_state=42)

# Function to check model effectiveness
def check_model_effectiveness(model, X_train, X_test, y_train, y_test, features_scaled, target, model_name="Model"):
    # Train the model
    model.fit(X_train, y_train)
    
    # Make predictions
    y_pred = model.predict(X_test)
    
    # Calculate metrics
    mse = mean_squared_error(y_test, y_pred)
    r2 = r2_score(y_test, y_pred)
    
    # Cross-Validation for robustness
    cv_scores = cross_val_score(model, features_scaled, target, cv=5, scoring='r2')
    
    print(f"{model_name} - MSE: {mse:.2f}, R²: {r2:.2f}, Cross-Validation R²: {cv_scores.mean():.2f}")
    
    return mse, r2, cv_scores.mean()

# Linear Regression Model
lr = LinearRegression()
mse_lr, r2_lr, cv_lr = check_model_effectiveness(lr, X_train, X_test, y_train, y_test, features_scaled, target, "Linear Regression")

# Random Forest Regressor with Hyperparameter Tuning
rf = RandomForestRegressor(random_state=42)
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [10, 20, None],
    'min_samples_split': [2, 5],
    'min_samples_leaf': [1, 2]
}

grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)
grid_search.fit(X_train, y_train)

best_rf = grid_search.best_estimator_
mse_rf, r2_rf, cv_rf = check_model_effectiveness(best_rf, X_train, X_test, y_train, y_test, features_scaled, target, "Random Forest")

# Feature Importance
feature_importance = best_rf.feature_importances_
sorted_idx = np.argsort(feature_importance)

plt.figure(figsize=(10, 6))
plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], align='center')
plt.yticks(range(len(sorted_idx)), [features.columns[i] for i in sorted_idx])
plt.xlabel('Feature Importance')
plt.title('Random Forest Feature Importance')
plt.show()

# Actual vs Predicted Prices for Random Forest
plt.figure(figsize=(10, 6))
plt.scatter(y_test, y_pred_rf, alpha=0.6)
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='red', linestyle='--')
plt.xlabel('Actual Prices')
plt.ylabel('Predicted Prices')
plt.title('Actual vs Predicted Prices - Random Forest')
plt.show()

# Residual Analysis
residuals = y_test - y_pred_rf
plt.figure(figsize=(10, 6))
sns.histplot(residuals, kde=True)
plt.title('Residuals Distribution - Random Forest')
plt.xlabel('Residuals')
plt.show()
