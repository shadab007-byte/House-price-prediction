{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxq/XyxUtTSKjpSCohuBls",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shadab007-byte/House-price-prediction/blob/main/Untitled4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5bqZPKoJcmS",
        "outputId": "8132b318-e968-45d0-a746-cdb0a24a7e46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       102\n",
            "           1       1.00      1.00      1.00       102\n",
            "\n",
            "    accuracy                           1.00       204\n",
            "   macro avg       1.00      1.00      1.00       204\n",
            "weighted avg       1.00      1.00      1.00       204\n",
            "\n",
            "Accuracy: 1.0000\n",
            "Cross-Validation Accuracy: 1.0000\n"
          ]
        }
      ],
      "source": [
        "# Required Libraries\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Custom Transformer for text preprocessing\n",
        "class TextCleaner(BaseEstimator, TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        return X.apply(self.clean_text)\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'http\\S+', '', text)\n",
        "        text = re.sub(r'@\\w+', '', text)\n",
        "        text = re.sub(r'#\\w+', '', text)\n",
        "        text = re.sub(r'\\d+', '', text)\n",
        "        text = re.sub(r'\\W', ' ', text)\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        tokens = [lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words]\n",
        "        return ' '.join(tokens)\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('large_sample_tweets.csv')  # Update path as needed\n",
        "X = df['tweet']\n",
        "y = df['label']  # Update label column as needed\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Pipeline with TF-IDF and Random Forest Classifier\n",
        "pipeline = Pipeline([\n",
        "    ('cleaner', TextCleaner()),\n",
        "    ('vectorizer', TfidfVectorizer(max_features=5000, ngram_range=(1, 2), sublinear_tf=True)),\n",
        "    ('classifier', RandomForestClassifier(random_state=42))\n",
        "])\n",
        "\n",
        "# Hyperparameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    'classifier__n_estimators': [100, 200, 300],\n",
        "    'classifier__max_depth': [None, 10, 20, 30],\n",
        "    'classifier__min_samples_split': [2, 5, 10],\n",
        "    'classifier__min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# GridSearch with Cross-Validation\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Best model from GridSearch\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Cross-validation accuracy\n",
        "cv_scores = cross_val_score(best_model, X_train, y_train, cv=5, scoring='accuracy')\n",
        "print(f\"Cross-Validation Accuracy: {cv_scores.mean():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lbX5ibHNCOGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yh8zojP8CQKS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Required Libraries\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('large_sample_tweets.csv')  # Using the specified file\n",
        "\n",
        "# Preprocessing function to clean tweets\n",
        "def clean_text(text):\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
        "    text = re.sub(r'@\\w+', '', text)  # Remove mentions\n",
        "    text = re.sub(r'#\\w+', '', text)  # Remove hashtags\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "    text = re.sub(r'\\W', ' ', text)  # Remove special characters\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
        "    tokens = [word for word in text.split() if word not in stop_words]  # Remove stopwords\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Apply preprocessing to the 'tweet' column\n",
        "df['clean_text'] = df['tweet'].apply(clean_text)\n",
        "\n",
        "# Defining features (X) and labels (y)\n",
        "X = df['clean_text']\n",
        "y = df['label']  # Assuming the label column is named 'label'\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Using TF-IDF vectorization for feature extraction\n",
        "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))  # Using bigrams for more context\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Random Forest Classifier with GridSearch for hyperparameter tuning\n",
        "rf = RandomForestClassifier()\n",
        "\n",
        "# Set up the hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Perform GridSearch to find the best parameters\n",
        "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
        "grid_search.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Best Random Forest model\n",
        "best_rf = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = best_rf.predict(X_test_tfidf)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Check the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Cross-validation accuracy\n",
        "cv_scores = cross_val_score(best_rf, X_train_tfidf, y_train, cv=5, scoring='accuracy')\n",
        "print(f\"Cross-Validation Accuracy: {cv_scores.mean():.4f}\")\n",
        "\n",
        "# Evaluation with examples\n",
        "import numpy as np\n",
        "predictions = best_rf.predict(X_test_tfidf)\n",
        "\n",
        "# Convert test set to DataFrame for better display\n",
        "test_results = pd.DataFrame({\n",
        "    'Tweet': X_test,\n",
        "    'True Label': y_test,\n",
        "    'Predicted Label': predictions\n",
        "})\n",
        "\n",
        "# Show examples of correctly classified hate speech\n",
        "print(\"Correctly Classified Hate Speech Examples:\")\n",
        "print(test_results[(test_results['True Label'] == 1) & (test_results['Predicted Label'] == 1)].head(5))\n",
        "\n",
        "# Show examples of incorrectly classified hate speech\n",
        "print(\"\\nIncorrectly Classified Hate Speech Examples:\")\n",
        "print(test_results[(test_results['True Label'] == 1) & (test_results['Predicted Label'] == 0)].head(5))\n",
        "\n",
        "# Show examples of correctly classified non-hate speech\n",
        "print(\"\\nCorrectly Classified Non-Hate Speech Examples:\")\n",
        "print(test_results[(test_results['True Label'] == 0) & (test_results['Predicted Label'] == 0)].head(5))\n",
        "\n",
        "# Show examples of incorrectly classified non-hate speech\n",
        "print(\"\\nIncorrectly Classified Non-Hate Speech Examples:\")\n",
        "print(test_results[(test_results['True Label'] == 0) & (test_results['Predicted Label'] == 1)].head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7Oess5bMqba",
        "outputId": "9891c33c-19e9-46b1-a368-f5c865c684c5"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       102\n",
            "           1       1.00      1.00      1.00       102\n",
            "\n",
            "    accuracy                           1.00       204\n",
            "   macro avg       1.00      1.00      1.00       204\n",
            "weighted avg       1.00      1.00      1.00       204\n",
            "\n",
            "Accuracy: 1.0000\n",
            "Cross-Validation Accuracy: 1.0000\n",
            "Correctly Classified Hate Speech Examples:\n",
            "                      Tweet  True Label  Predicted Label\n",
            "523      wish people better           1                1\n",
            "526   anger consumes around           1                1\n",
            "76           focus positive           1                1\n",
            "70   appreciate good around           1                1\n",
            "675            despise much           1                1\n",
            "\n",
            "Incorrectly Classified Hate Speech Examples:\n",
            "Empty DataFrame\n",
            "Columns: [Tweet, True Label, Predicted Label]\n",
            "Index: []\n",
            "\n",
            "Correctly Classified Non-Hate Speech Examples:\n",
            "                                     Tweet  True Label  Predicted Label\n",
            "602                      kindness everyone           0                0\n",
            "31                      community positive           0                0\n",
            "616  love kindness make world better place           0                0\n",
            "585                             fills rage           0                0\n",
            "444              choose kindness every day           0                0\n",
            "\n",
            "Incorrectly Classified Non-Hate Speech Examples:\n",
            "Empty DataFrame\n",
            "Columns: [Tweet, True Label, Predicted Label]\n",
            "Index: []\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Required Libraries\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Load the new challenging dataset\n",
        "df = pd.read_csv('large_sample_tweets.csv')  # Update path as needed\n",
        "\n",
        "# Preprocessing function to clean tweets\n",
        "def clean_text(text):\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
        "    text = re.sub(r'@\\w+', '', text)  # Remove mentions\n",
        "    text = re.sub(r'#\\w+', '', text)  # Remove hashtags\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "    text = re.sub(r'\\W', ' ', text)  # Remove special characters\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
        "    tokens = [word for word in text.split() if word not in stop_words]  # Remove stopwords\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Apply preprocessing to the 'tweet' column\n",
        "df['clean_text'] = df['tweet'].apply(clean_text)\n",
        "\n",
        "# Defining features (X) and labels (y)\n",
        "X = df['clean_text']\n",
        "y = df['label']  # Assuming the label column is named 'label'\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Using TF-IDF vectorization for feature extraction\n",
        "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))  # Using bigrams for more context\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "# Random Forest Classifier with GridSearch for hyperparameter tuning\n",
        "rf = RandomForestClassifier()\n",
        "\n",
        "# Set up the hyperparameter grid\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Perform GridSearch to find the best parameters\n",
        "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
        "grid_search.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# Best Random Forest model\n",
        "best_rf = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = best_rf.predict(X_test_tfidf)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Check the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Cross-validation accuracy\n",
        "cv_scores = cross_val_score(best_rf, X_train_tfidf, y_train, cv=5, scoring='accuracy')\n",
        "print(f\"Cross-Validation Accuracy: {cv_scores.mean():.4f}\")\n",
        "\n",
        "# Check correct and incorrect classifications\n",
        "correct_hate_speech = df[(y_test == 1) & (y_pred == 1)]\n",
        "incorrect_hate_speech = df[(y_test == 1) & (y_pred == 0)]\n",
        "correct_non_hate_speech = df[(y_test == 0) & (y_pred == 0)]\n",
        "incorrect_non_hate_speech = df[(y_test == 0) & (y_pred == 1)]\n",
        "\n",
        "# Output results\n",
        "print(\"\\nCorrectly Classified Hate Speech Examples:\")\n",
        "print(correct_hate_speech[['tweet', 'label', 'predicted_label']])\n",
        "print(\"\\nIncorrectly Classified Hate Speech Examples:\")\n",
        "print(incorrect_hate_speech[['tweet', 'label', 'predicted_label']])\n",
        "\n",
        "print(\"\\nCorrectly Classified Non-Hate Speech Examples:\")\n",
        "print(correct_non_hate_speech[['tweet', 'label', 'predicted_label']])\n",
        "print(\"\\nIncorrectly Classified Non-Hate Speech Examples:\")\n",
        "print(incorrect_non_hate_speech[['tweet', 'label', 'predicted_label']])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 706
        },
        "id": "s6bQDVEdNzue",
        "outputId": "d1d8e7a0-2950-45c4-e719-6e405ce85565"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       102\n",
            "           1       1.00      1.00      1.00       102\n",
            "\n",
            "    accuracy                           1.00       204\n",
            "   macro avg       1.00      1.00      1.00       204\n",
            "weighted avg       1.00      1.00      1.00       204\n",
            "\n",
            "Accuracy: 1.0000\n",
            "Cross-Validation Accuracy: 1.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-65a8108d7a1e>:79: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
            "  correct_hate_speech = df[(y_test == 1) & (y_pred == 1)]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexingError",
          "evalue": "Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexingError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-65a8108d7a1e>\u001b[0m in \u001b[0;36m<cell line: 79>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;31m# Check correct and incorrect classifications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m \u001b[0mcorrect_hate_speech\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0mincorrect_hate_speech\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0mcorrect_non_hate_speech\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m&\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4091\u001b[0m         \u001b[0;31m# Do we have a (boolean) 1d indexer?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4092\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4093\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_bool_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4095\u001b[0m         \u001b[0;31m# We are left with two options: a single key, and a collection of keys,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_bool_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4147\u001b[0m         \u001b[0;31m# check_bool_indexer will throw exception if Series key cannot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4148\u001b[0m         \u001b[0;31m# be reindexed to match DataFrame rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4149\u001b[0;31m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4151\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36mcheck_bool_indexer\u001b[0;34m(index, key)\u001b[0m\n\u001b[1;32m   2660\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer_for\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2661\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2662\u001b[0;31m             raise IndexingError(\n\u001b[0m\u001b[1;32m   2663\u001b[0m                 \u001b[0;34m\"Unalignable boolean Series provided as \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;34m\"indexer (index of the boolean Series and of \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexingError\u001b[0m: Unalignable boolean Series provided as indexer (index of the boolean Series and of the indexed object do not match)."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "DSZNDEMJJgpg"
      }
    }
  ]
}